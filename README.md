MNIST Handwritten Digit Recognition ProjectThis project implements a fundamental Feedforward Neural Network to classify handwritten digits from the MNIST dataset. It serves as an introductory practical application of deep learning algorithms, specifically focusing on Stochastic Gradient Descent (SGD) and Backpropagation.The core implementation is based on the foundational Python/NumPy code provided in Michael Nielsen's "Neural Networks and Deep Learning" book.üéØ Project Goal & ScopeThe primary objectives of this project were:To successfully implement and train a Sigmoid-based neural network on the MNIST dataset.To experimentally perform hyperparameter tuning by isolating and testing the impact of three core parameters on classification accuracy.üõ†Ô∏è Model Architecture and AlgorithmFeatureDetailModel TypeFully Connected Network (MLP)Neuron ActivationSigmoid function ($\sigma(z)$) for all layers.Learning AlgorithmMini-Batch Stochastic Gradient Descent (SGD).Cost FunctionQuadratic Cost (Mean Squared Error, MSE).Input Layer784 neurons ($28 \times 28$ pixel image vector).Hidden Layer30 neurons (Baseline).Output Layer10 neurons (One-Hot Encoding for digits 0-9).üìä Hyperparameter Tuning AnalysisThe following table summarizes the epoch-by-epoch test accuracy results for the Baseline model and the three isolated hyperparameter experiments.Deney NoParameter ChangedBaseline SettingNew SettingFinal Accuracy (Epoch 29)Primary ObservationBaseline(30N, $\eta=3.0$, $m=10$)--$\approx \mathbf{94.78\%}$The model converges effectively and rapidly due to the large learning rate ($\eta$).Deney 1Hidden Layer Neurons30100$\approx \mathbf{96.57\%}$Significant Improvement. Increased model capacity allows the network to learn more complex features, leading to higher accuracy, despite a slower start.Deney 2Learning Rate ($\eta$)3.00.5$\approx \mathbf{94.02\%}$Performance Drop. The smaller step size leads to slow convergence, preventing the model from reaching the optimal performance of the Baseline within the 30-epoch limit.Deney 3Mini-Batch Size ($m$)1032$\approx \mathbf{94.58\%}$Marginal Effect. The larger batch size provides a more stable (less noisy) gradient estimate, but the reduced number of updates per epoch causes a slight underperformance compared to the Baseline ($m=10$).
